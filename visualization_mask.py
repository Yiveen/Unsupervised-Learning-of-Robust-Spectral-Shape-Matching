import os.path
import os
import numpy as np

import torch
import matplotlib.pyplot as plt

import datetime
import sys
import math
import time
from os import path as osp

import torch.cuda

from datasets import build_dataloader, build_dataset
from datasets.data_sampler import EnlargedSampler

from models import build_model
from utils import (AvgTimer, MessageLogger, get_env_info, get_root_logger,
                   init_tb_logger)
from utils.options import dict2str, parse_options

def _get_mask(evals1, evals2, resolvant_gamma):
    scaling_factor = max(torch.max(evals1), torch.max(evals2))
    evals1, evals2 = evals1 / scaling_factor, evals2 / scaling_factor
    evals_gamma1 = (evals1 ** resolvant_gamma)[None, :]
    evals_gamma2 = (evals2 ** resolvant_gamma)[:, None]

    M_re = evals_gamma2 / (evals_gamma2.square() + 1) - evals_gamma1 / (evals_gamma1.square() + 1)
    M_im = 1 / (evals_gamma2.square() + 1) - 1 / (evals_gamma1.square() + 1)
    return M_re.square() + M_im.square()

def create_train_val_dataloader(opt, logger):
    train_set, val_set = None, None
    # create train and val datasets
    for dataset_name, dataset_opt in opt['datasets'].items():
        if isinstance(dataset_opt, int):  # batch_size, num_worker
            continue
        if dataset_name.startswith('train'):
            dataset_enlarge_ratio = dataset_opt.get('dataset_enlarge_ratio', 1)
            if train_set is None:
                train_set = build_dataset(dataset_opt)
            else:
                train_set += build_dataset(dataset_opt)
        elif dataset_name.startswith('val') or dataset_name.startswith('test'):
            if val_set is None:
                val_set = build_dataset(dataset_opt)
            else:
                val_set += build_dataset(dataset_opt)

    # create train and val dataloaders
    train_sampler = EnlargedSampler(train_set, opt['world_size'], opt['rank'], dataset_enlarge_ratio)
    train_loader = build_dataloader(
        train_set,
        opt['datasets'],
        'train',
        num_gpu=opt['num_gpu'],
        dist=opt['dist'],
        sampler=train_sampler,
        seed=opt['manual_seed'])
    batch_size = opt['datasets']['batch_size']
    num_iter_per_epoch = math.ceil(
        len(train_set) * dataset_enlarge_ratio / batch_size)
    total_epochs = int(opt['train']['total_epochs'])
    total_iters = total_epochs * num_iter_per_epoch
    logger.info('Training statistics:'
                f'\n\tNumber of train images: {len(train_set)}'
                f'\n\tDataset enlarge ratio: {dataset_enlarge_ratio}'
                f'\n\tBatch size: {batch_size}'
                f'\n\tWorld size (gpu number): {opt["world_size"]}'
                f'\n\tRequire iter number per epoch: {num_iter_per_epoch}'
                f'\n\tTotal epochs: {total_epochs}; iters: {total_iters}.')

    val_loader = build_dataloader(
        val_set, opt['datasets'], 'val', num_gpu=opt['num_gpu'], dist=opt['dist'], sampler=None,
        seed=opt['manual_seed'])
    logger.info('Validation statistics:'
                f'\n\tNumber of val images: {len(val_set)}')

    return train_loader, train_sampler, val_loader, total_epochs, total_iters


# 创建模拟数据
torch.manual_seed(0)  # 为了结果的可重复性
resolvant_gamma = 0.5  # 选择一个gamma值
base_path = '/home/yiwen/TUM/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching/data/FAUST/graph/graphs_evals'

evals1 = np.load(os.path.join(base_path, 'tr_reg_001.npy'))
evals2 = np.load(os.path.join(base_path, 'tr_reg_001.npy'))
evals_x = torch.from_numpy(evals1).squeeze()
evals_x = torch.flip(evals_x, dims=[0])
evals_y = torch.from_numpy(evals2).squeeze()
evals_y = torch.flip(evals_y, dims=[0])


# # parse options, set distributed setting, set random seed
# root_path = osp.abspath(osp.join(__file__, osp.pardir))
# opt = parse_options(root_path, is_train=True)
# opt['root_path'] = root_path
#
# # WARNING: should not use get_root_logger in the above codes, including the called functions
# # Otherwise the logger will not be properly initialized
# log_file = osp.join(opt['path']['log'], f"train_{opt['name']}.log")
# logger = get_root_logger(log_file=log_file)
# logger.info(get_env_info())
# logger.info(dict2str(opt))
#
# # create train and validation dataloaders
# result = create_train_val_dataloader(opt, logger)
# train_loader, train_sampler, val_loader, total_epochs, total_iters = result
#
# for data in train_loader:
#     data_x, data_y = data['first'], data['second']
#     break
# # get spectral operators
# evals_x = data_x['evals'][0]
# evals_y = data_y['evals'][0]

# 生成mask
mask = _get_mask(evals_x, evals_y, resolvant_gamma)

plt.figure(figsize=(10, 8))
plt.imshow(mask.numpy(), cmap='viridis')  # 使用viridis颜色映射来增强视觉效果
plt.colorbar()  # 显示颜色条
plt.title('Visualization of Mask Generated by _get_mask')
plt.xlabel('Index in evals1')
plt.ylabel('Index in evals2')
plt.show()
